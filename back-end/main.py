
"""Ø§Ù„Ù…Ù„Ù Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠ Ù„ØªØ·Ø¨ÙŠÙ‚ ÙˆØ§Ø¬Ù‡Ø© Ø¨Ø±Ù…Ø¬Ø© API Ø§Ù„Ù…Ø³Ø¤ÙˆÙ„ Ø¹Ù†:
1. Ø±ÙØ¹ Ù…Ù„ÙØ§Øª PDF ÙˆØ§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ù†ØµÙˆØµ.
2. Ø¨Ø« (Streaming) Ø§Ù„ØªÙ„Ø®ÙŠØµ Ø¹Ø¨Ø± SSE Ø¨Ø´ÙƒÙ„ ØªØ¯Ø±ÙŠØ¬ÙŠ Ø´Ø¨ÙŠÙ‡ ChatGPT.
3. Ø§Ù„ØªØ®Ø²ÙŠÙ† Ø§Ù„Ù…Ø¤Ù‚Øª Ù„Ù„Ù†ØµÙˆØµ ÙˆØ§Ù„ØªÙ„Ø®ÙŠØµØ§Øª Ù„ØªÙ‚Ù„ÙŠÙ„ Ø§Ù„Ø²Ù…Ù† Ø¹Ù†Ø¯ Ø§Ù„ØªÙƒØ±Ø§Ø±.
4. ØªÙ‡ÙŠØ¦Ø© Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ (Warmup) Ù„ØªÙ‚Ù„ÙŠÙ„ ØªØ£Ø®ÙŠØ± Ø£ÙˆÙ„ Ø§Ø³ØªØ¬Ø§Ø¨Ø©.
"""

# ============== Ø§Ù„Ø¥Ø³ØªÙŠØ±Ø§Ø¯Ø§Øª Ø§Ù„Ø£Ø³Ø§Ø³ÙŠØ© ==============
import uuid  # Ù„Ø¥Ù†Ø´Ø§Ø¡ Ù…Ø¹Ø±ÙØ§Øª ÙØ±ÙŠØ¯Ø© Ù„Ù„Ø¬Ù„Ø³Ø§Øª ÙˆØ§Ù„Ù…Ù„ÙØ§Øª Ø§Ù„Ù…Ø¤Ù‚ØªØ©
import time  # Ù„Ø­Ø³Ø§Ø¨ ÙØªØ±Ø§Øª Ø¥Ù†ØªÙ‡Ø§Ø¡ Ø§Ù„ØµÙ„Ø§Ø­ÙŠØ© (TTL)
import asyncio  # Ù„ØªÙ†ÙÙŠØ° Ø¹Ù…Ù„ÙŠØ§Øª Ø§Ù„Ø¥Ø¯Ø®Ø§Ù„/Ø§Ù„Ø¥Ø®Ø±Ø§Ø¬ Ø¨Ø´ÙƒÙ„ ØºÙŠØ± Ù…ØªØ²Ø§Ù…Ù†
import hashlib  # Ù„Ø¥Ù†Ø´Ø§Ø¡ Ù…ÙØ§ØªÙŠØ­ ØªØ¬Ø²Ø¦Ø© (Hash) ÙØ±ÙŠØ¯Ø© Ù…Ù† Ù…Ø­ØªÙˆÙ‰ Ø§Ù„Ù†Øµ
from pathlib import Path  # Ù„Ù„ØªØ¹Ø§Ù…Ù„ Ù…Ø¹ Ø§Ù„Ù…Ø³Ø§Ø±Ø§Øª Ø¨Ø´ÙƒÙ„ Ø¢Ù…Ù†
from typing import AsyncGenerator  # Ù„ØªØ­Ø¯ÙŠØ¯ Ù†ÙˆØ¹ Ø§Ù„Ø¯ÙˆØ§Ù„ Ø§Ù„Ù…ÙˆÙ„Ù‘ÙØ¯Ø© ØºÙŠØ± Ø§Ù„Ù…ØªØ²Ø§Ù…Ù†Ø©
import logging

import PyPDF2  # Ù…ÙƒØªØ¨Ø© Ù„Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ù†ØµÙˆØµ Ù…Ù† Ù…Ù„ÙØ§Øª PDF
from fastapi import FastAPI, UploadFile, HTTPException, Query  # FastAPI Ù„Ø¨Ù†Ø§Ø¡ ÙˆØ§Ø¬Ù‡Ø© Ø§Ù„API
from fastapi.responses import StreamingResponse, JSONResponse  # Ù„Ù„Ø±Ø¯ Ø¨Ø¨Ø« SSE Ø£Ùˆ JSON Ø¹Ø§Ø¯ÙŠ
from fastapi.middleware.cors import CORSMiddleware
import pypdfium2 

## ØªÙ… Ø§Ù„Ø§Ø³ØªØºÙ†Ø§Ø¡ Ø¹Ù† OllamaØŒ Ø§Ù„Ø±Ø¨Ø· Ø§Ù„Ø¢Ù† Ù…Ø¹ Gemini ÙÙ‚Ø·
from uploads.config import (
    MAX_PDF_SIZE,
    DEFAULT_MODEL,
    gemini_models,
    client,
    FRONTEND_ORIGINS,
    ALLOW_ORIGIN_REGEX,
)

# ============== Ø¥Ù†Ø´Ø§Ø¡ ØªØ·Ø¨ÙŠÙ‚ FastAPI ==============
app = FastAPI(title="AI PDF Summarizer API", version="1.1.0")

# Ø¥Ø¹Ø¯Ø§Ø¯ Ù„ÙˆØ¬ Ø¨Ø³ÙŠØ· Ù…Ø¹ Ø§Ù„ÙˆÙ‚Øª ÙˆØ§Ù„Ù…Ø³ØªÙˆÙ‰
logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s %(levelname)s %(name)s: %(message)s",
)
log = logging.getLogger("ai-summary")
# Ø­Ø¯Ø« ØªØ´ØºÙŠÙ„ Ù…Ø¨Ø¯Ø¦ÙŠ Ù„Ù„ØªØ³Ø®ÙŠÙ† (Warmup) Ù„ØªØ³Ø±ÙŠØ¹ Ø£ÙˆÙ„ Ø§Ø³ØªØ¬Ø§Ø¨Ø©
@app.on_event("startup")
async def _warmup():
    """Warmup model with a tiny request to reduce first-token latency."""
    sample_text = "Ø§Ø®ØªØ¨Ø§Ø± ØªÙ…Ù‡ÙŠØ¯ÙŠ ØµØºÙŠØ±"  # very short
    try:
        # run sync streaming in executor to avoid blocking event loop
        def _do_warmup():
            stream = client.models.generate_content_stream(
                model=DEFAULT_MODEL,
                contents=[sample_text]
            )
            # consume a couple of chunks then stop
            taken = 0
            for chunk in stream:
                if getattr(chunk, "text", None):
                    taken += 1
                if taken >= 2:
                    break

        loop = asyncio.get_running_loop()
        await loop.run_in_executor(None, _do_warmup)
    except Exception:
        # ignore warmup errors
        pass
    
# ============== Ù‡ÙŠØ§ÙƒÙ„ Ø§Ù„ØªØ®Ø²ÙŠÙ† Ø¯Ø§Ø®Ù„ Ø§Ù„Ø°Ø§ÙƒØ±Ø© ==============
text_storage: dict[str, str] = {}  # ØªØ®Ø²ÙŠÙ† Ø§Ù„Ù†Øµ Ø§Ù„Ø®Ø§Ù… Ø§Ù„Ù…Ø³ØªØ®Ø±Ø¬ Ù„ÙƒÙ„ Ø¬Ù„Ø³Ø© session_id -> text
summary_cache: dict[str, tuple[float, str]] = {}  # ØªØ®Ø²ÙŠÙ† Ø§Ù„ØªÙ„Ø®ÙŠØµ Ø§Ù„Ù†Ù‡Ø§Ø¦ÙŠ (Ø²Ù…Ù† Ø§Ù„ØªØ®Ø²ÙŠÙ†, Ø§Ù„ØªÙ„Ø®ÙŠØµ)
CACHE_TTL = 60 * 10  # Ù…Ø¯Ø© ØµÙ„Ø§Ø­ÙŠØ© Ø§Ù„ØªÙ„Ø®ÙŠØµ ÙÙŠ Ø§Ù„Ø°Ø§ÙƒØ±Ø© (Ø«ÙˆØ§Ù†Ù) = 10 Ø¯Ù‚Ø§Ø¦Ù‚

# Ù…Ù‡Ø§Ù… Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ù‚ÙŠØ¯ Ø§Ù„ØªÙ†ÙÙŠØ° Ù„ÙƒÙ„ Ø¬Ù„Ø³Ø©
pending_extractions: dict[str, asyncio.Task] = {}

# Ù…Ø¬Ù„Ø¯ Ø§Ù„Ù…Ù„ÙØ§Øª Ø§Ù„Ù…Ø¤Ù‚ØªØ© (Ø§Ù„Ù†Ø³Ø® Ø§Ù„Ù…Ø±ÙÙˆØ¹Ø© Ù…Ù† Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…)
UPLOAD_DIR = Path("temp")
UPLOAD_DIR.mkdir(exist_ok=True)  # Ø¥Ù†Ø´Ø§Ø¡ Ø§Ù„Ù…Ø¬Ù„Ø¯ Ø¥Ù† Ù„Ù… ÙŠÙƒÙ† Ù…ÙˆØ¬ÙˆØ¯Ø§Ù‹

FILE_TTL_SECONDS = 60 * 30  # Ø­Ø°Ù Ù…Ù„ÙØ§Øª Ø§Ù„Ù€ PDF Ø§Ù„Ù…Ø¤Ù‚ØªØ© Ø§Ù„Ø£Ù‚Ø¯Ù… Ù…Ù† 30 Ø¯Ù‚ÙŠÙ‚Ø©

def _cleanup_old_files() -> None:
    """ØªÙ†Ø¸ÙŠÙ Ø§Ù„Ù…Ù„ÙØ§Øª Ø§Ù„Ù‚Ø¯ÙŠÙ…Ø© Ù…Ù† Ø§Ù„Ù…Ø¬Ù„Ø¯ Ø§Ù„Ù…Ø¤Ù‚Øª Ù„ØªÙˆÙÙŠØ± Ø§Ù„Ù…Ø³Ø§Ø­Ø©."""
    now = time.time()
    for p in UPLOAD_DIR.glob("*.pdf"):
        try:
            # Ø¥Ø°Ø§ Ù…Ø± ÙˆÙ‚Øª Ø£ÙƒØ¨Ø± Ù…Ù† Ø­Ø¯ TTL Ù†Ø­Ø°Ù Ø§Ù„Ù…Ù„Ù
            if now - p.stat().st_mtime > FILE_TTL_SECONDS:
                p.unlink(missing_ok=True)
        except OSError:
            # Ù†ØªØ¬Ø§Ù‡Ù„ Ø§Ù„Ø£Ø®Ø·Ø§Ø¡ (Ù…Ø«Ù„Ø§Ù‹ Ø¹Ù†Ø¯ Ø­Ø°Ù Ù…ØªØ²Ø§Ù…Ù† Ø£Ùˆ ØµÙ„Ø§Ø­ÙŠØ§Øª)
            pass


async def _extract_and_store(session_id: str, pdf_path: Path) -> None:
    """ØªØ´ØºÙŠÙ„ Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ù†Øµ ÙÙŠ Ù…Ù†ÙØ° (ThreadPool) Ø«Ù… ØªØ®Ø²ÙŠÙ†Ù‡ ÙˆØ­Ø°Ù Ø§Ù„Ù…Ù„Ù Ø§Ù„Ù…Ø¤Ù‚Øª."""
    start = time.time()
    loop = asyncio.get_running_loop()
    try:
        text = await loop.run_in_executor(None, extract_text_from_pdf, pdf_path)
        # Ø­Ø°Ù Ø§Ù„Ù…Ù„Ù Ø§Ù„Ù…Ø¤Ù‚Øª Ø¨Ø¹Ø¯ Ø§Ù„Ø§Ø³ØªØ®Ø±Ø§Ø¬
        try:
            pdf_path.unlink(missing_ok=True)
        except Exception:
            pass
        text_storage[session_id] = text or ""
        log.info("extracted text for %s in %.2fs (chars=%d)", session_id, time.time()-start, len(text or ""))
    except Exception as e:
        text_storage[session_id] = ""
        log.exception("failed to extract text for %s: %s", session_id, e)
    finally:
        pending_extractions.pop(session_id, None)

# ============== Ø¥Ø¹Ø¯Ø§Ø¯ CORS Ù„Ù„Ø³Ù…Ø§Ø­ Ù„ØªØ·Ø¨ÙŠÙ‚ Ø§Ù„ÙˆØ§Ø¬Ù‡Ø© Ø§Ù„Ø£Ù…Ø§Ù…ÙŠØ© Ø¨Ø§Ù„Ø§ØªØµØ§Ù„ ==============
app.add_middleware(
    CORSMiddleware,
    allow_origins=FRONTEND_ORIGINS,
    allow_origin_regex=ALLOW_ORIGIN_REGEX or None,
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)


# def extract_text_from_pdf(file_path: Path) -> str:
#     """Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ù†Øµ Ù…Ù† Ù…Ù„Ù PDF ØµÙØ­Ø© Ø¨ØµÙØ­Ø©.

#     Ù…Ù„Ø§Ø­Ø¸Ø§Øª:
#     - PyPDF2 Ù‚Ø¯ ÙŠÙØ´Ù„ ÙÙŠ Ø¨Ø¹Ø¶ Ø§Ù„ØµÙØ­Ø§Øª Ø§Ù„ØªØ§Ù„ÙØ©ØŒ Ù„Ø°Ø§ Ù†Ø­Ø§ÙˆÙ„ ÙˆÙ†ØªØ¬Ø§Ù‡Ù„ Ø§Ù„ØµÙØ­Ø© Ø¥Ù† ÙØ´Ù„.
#     - ÙŠØªÙ… Ø¯Ù…Ø¬ Ø§Ù„Ù†ØµÙˆØµ ÙÙŠ Ø³Ù„Ø³Ù„Ø© ÙˆØ§Ø­Ø¯Ø© Ù…ÙØµÙˆÙ„Ø© Ø¨Ø£Ø³Ø·Ø± Ø¬Ø¯ÙŠØ¯Ø©.
#     - ÙŠÙ…ÙƒÙ† Ù„Ø§Ø­Ù‚Ø§Ù‹ ØªØ­Ø³ÙŠÙ† Ø§Ù„Ø£Ø¯Ø§Ø¡ Ø¨ØªÙ‚Ø³ÙŠÙ… Ù…Ø­ØªÙˆÙ‰ ÙƒØ¨ÙŠØ± Ø¥Ù„Ù‰ Ø£Ø¬Ø²Ø§Ø¡ Ù„Ù„ØªÙ„Ø®ÙŠØµ Ø§Ù„Ù…Ø±Ø­Ù„ÙŠ.
#     """
#     reader = PyPDF2.PdfReader(str(file_path))
#     parts: list[str] = []  # ØªØ¬Ù…ÙŠØ¹ Ø§Ù„Ù†ØµÙˆØµ Ù‡Ù†Ø§
#     for page in reader.pages:
#         try:
#             page_text = page.extract_text() or ""  # Ù‚Ø¯ ØªØ¹ÙŠØ¯ Ø§Ù„Ø¯Ø§Ù„Ø© None
#         except Exception:  # ÙÙŠ Ø­Ø§Ù„Ø© ØµÙØ­Ø© ØªØ§Ù„ÙØ© Ø£Ùˆ Ø®Ø·Ø£ Ø¯Ø§Ø®Ù„ÙŠ
#             page_text = ""
#         if page_text:
#             parts.append(page_text)
#     return "\n".join(parts).strip()  # Ø¯Ù…Ø¬ ÙƒÙ„ Ø§Ù„ØµÙØ­Ø§Øª ÙÙŠ Ù†Øµ ÙˆØ§Ø­Ø¯

def extract_text_from_pdf(file_path: Path) -> str:
    pdf = pypdfium2.PdfDocument(file_path)
    return "\n".join(page.get_textpage().get_text_range() for page in pdf)


def _encode_sse_chunk(text: str) -> bytes:
    """ØªÙ‡ÙŠØ¦Ø© Ù†Øµ Ø®Ø§Ù… Ù„ÙŠÙƒÙˆÙ† Ù…ØªÙˆØ§ÙÙ‚Ø§Ù‹ Ù…Ø¹ ØµÙŠØºØ© SSE (data: ...)."""
    if not text:
        return b"data: \n\n"
    sanitized = text.replace("\r\n", "\n").replace("\r", "\n")
    lines = sanitized.split("\n")
    payload = "".join(f"data: {line}\n" for line in lines)
    return (payload + "\n").encode("utf-8")



@app.post("/upload")
async def upload_pdf(file: UploadFile):
    """Ù†Ù‚Ø·Ø© Ù†Ù‡Ø§ÙŠØ© Ù„Ø±ÙØ¹ Ù…Ù„Ù PDF Ø«Ù… Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ù†Øµ ÙˆØªØ®Ø²ÙŠÙ†Ù‡ ÙˆØ¥Ø±Ø¬Ø§Ø¹ session_id.

    Ø®Ø·ÙˆØ§Øª Ø§Ù„ØªÙ†ÙÙŠØ°:
    1. Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† Ù†ÙˆØ¹ Ø§Ù„Ù…Ù„Ù.
    2. Ù‚Ø±Ø§Ø¡Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª ÙÙŠ Ø§Ù„Ø°Ø§ÙƒØ±Ø© ÙˆØ§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† Ø§Ù„Ø­Ø¬Ù….
    3. Ø­ÙØ¸ Ù†Ø³Ø®Ø© Ù…Ø¤Ù‚ØªØ© Ø¹Ù„Ù‰ Ø§Ù„Ù‚Ø±Øµ.
    4. Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ù†Øµ Ù…Ù† Ø§Ù„Ù†Ø³Ø®Ø© Ø§Ù„Ù…Ø¤Ù‚ØªØ©.
    5. ØªÙˆÙ„ÙŠØ¯ Ù…Ø¹Ø±Ù Ø¬Ù„Ø³Ø© ÙˆØªØ®Ø²ÙŠÙ† Ø§Ù„Ù†Øµ ÙÙŠ Ø§Ù„Ø°Ø§ÙƒØ±Ø©.
    6. Ø¥Ø¹Ø§Ø¯Ø© Ø§Ù„Ù…Ø¹Ø±Ù Ù„Ù„ÙˆØ§Ø¬Ù‡Ø© Ø§Ù„Ø£Ù…Ø§Ù…ÙŠØ© Ù„Ø·Ù„Ø¨ Ø§Ù„ØªÙ„Ø®ÙŠØµ Ù„Ø§Ø­Ù‚Ø§Ù‹.
    """
    if file.content_type not in {"application/pdf", "application/octet-stream"}:
        raise HTTPException(status_code=415, detail="Unsupported file type. Must be PDF.")  # Ù†ÙˆØ¹ ØºÙŠØ± Ù…Ø¯Ø¹ÙˆÙ…

    data = await file.read()  # Ù‚Ø±Ø§Ø¡Ø© ÙƒÙ„ Ø§Ù„Ù…Ù„Ù Ø¯Ø§Ø®Ù„ Ø§Ù„Ø°Ø§ÙƒØ±Ø©
    if len(data) > MAX_PDF_SIZE:
        raise HTTPException(status_code=413, detail="File too large.")  # Ø§Ù„Ù…Ù„Ù Ø£ÙƒØ¨Ø± Ù…Ù† Ø§Ù„Ø­Ø¯ Ø§Ù„Ù…Ø³Ù…ÙˆØ­

    # Ø§Ø³Ù… ÙØ±ÙŠØ¯ Ù„Ù„Ù…Ù„Ù Ø§Ù„Ù…Ø¤Ù‚Øª
    session_id = str(uuid.uuid4())
    pdf_path = UPLOAD_DIR / f"{session_id}.pdf"

    # ÙƒØªØ§Ø¨Ø© Ø§Ù„Ù…Ù„Ù Ø¹Ù„Ù‰ Ø§Ù„Ù‚Ø±Øµ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ù…Ù†ÙØ° (Executor) Ù„ØªØ¬Ù†Ø¨ Ø­Ø¸Ø± Ø§Ù„Ø­Ø¯Ø« Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠ
    try:
        loop = asyncio.get_running_loop()
        await loop.run_in_executor(None, pdf_path.write_bytes, data)
    except PermissionError:
        raise HTTPException(
            status_code=500,
            detail=(
                f"Server error: Permission denied to save file. "
                f"Check server user permissions on {UPLOAD_DIR} directory."
            ),
        )
    except Exception as e:
        raise HTTPException(
            status_code=500,
            detail=f"Server I/O error during file save: {type(e).__name__}.",
        )

    # Ø¨Ø¯Ø¡ Ø§Ù„Ø§Ø³ØªØ®Ø±Ø§Ø¬ ÙÙŠ Ø§Ù„Ø®Ù„ÙÙŠØ© Ù„ØªÙ‚Ù„ÙŠÙ„ Ø²Ù…Ù† Ø§Ù†ØªØ¸Ø§Ø± Ø§Ù„Ø¹Ù…ÙŠÙ„
    task = asyncio.create_task(_extract_and_store(session_id, pdf_path))
    pending_extractions[session_id] = task
    _cleanup_old_files()  # ØªÙ†Ø¸ÙŠÙ Ø§Ù„Ù…Ù„ÙØ§Øª Ø§Ù„Ù‚Ø¯ÙŠÙ…Ø© Ø¨Ø´ÙƒÙ„ Ø¯ÙˆØ±ÙŠ
    log.info("accepted upload session=%s size=%.2fKB", session_id, len(data)/1024)
    # Ù†Ø¹ÙŠØ¯ session_id ÙÙˆØ±Ø§Ù‹Ø› Ø§Ù„ÙˆØ§Ø¬Ù‡Ø© Ø³ØªÙØªØ­ SSE ÙˆØ³ÙŠØªÙ… Ø§Ù„Ø§Ù†ØªØ¸Ø§Ø± Ù‡Ù†Ø§Ùƒ Ø¥Ù† Ù„Ù… ÙŠÙƒØªÙ…Ù„ Ø§Ù„Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø¨Ø¹Ø¯
    return {"session_id": session_id, "characters": 0}




# ØªÙ… Ø¥Ù„ØºØ§Ø¡ Ù†Ù‚Ø·Ø© /stream Ø§Ù„Ù‚Ø¯ÙŠÙ…Ø©. Ø§Ø³ØªØ®Ø¯Ù… /summarize-gemini ÙÙ‚Ø·.

@app.get("/health")
async def health():
    """ÙØ­Øµ Ø­Ø§Ù„Ø© Ø§Ù„Ø®Ø¯Ù…Ø© ÙˆØ¥Ø±Ø¬Ø§Ø¹ Ø¹Ø¯Ø¯ Ø§Ù„Ø¬Ù„Ø³Ø§Øª Ø§Ù„Ø­Ø§Ù„ÙŠØ© ÙÙŠ Ø§Ù„Ø°Ø§ÙƒØ±Ø©."""
    return {"status": "ok", "sessions": len(text_storage)}

@app.get("/models")
async def models():
    """Ø¥Ø±Ø¬Ø§Ø¹ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ø§ÙØªØ±Ø§Ø¶ÙŠ ÙˆÙ‚Ø§Ø¦Ù…Ø© Ø§Ù„Ù†Ù…Ø§Ø°Ø¬ Ø§Ù„Ù…Ø¯Ø¹ÙˆÙ…Ø© Ù„Ù„Ø§Ø®ØªÙŠØ§Ø± Ù…Ù† Ø§Ù„ÙˆØ§Ø¬Ù‡Ø©."""
    return {"default": DEFAULT_MODEL, "models":gemini_models}

@app.delete("/session/{session_id}")
async def delete_session(session_id: str):
    """Ø­Ø°Ù Ø¬Ù„Ø³Ø© ÙˆØªÙØ±ÙŠØº Ø§Ù„Ù†Øµ Ø§Ù„Ù…Ø®Ø²Ù† Ø§Ù„Ø®Ø§Øµ Ø¨Ù‡Ø§ Ù…Ù† Ø§Ù„Ø°Ø§ÙƒØ±Ø© Ù„ØªÙ‚Ù„ÙŠÙ„ Ø§Ù„Ø§Ø³ØªØ®Ø¯Ø§Ù…."""
    removed = text_storage.pop(session_id, None)
    return JSONResponse({"removed": bool(removed)})


@app.get("/summarize-gemini")
async def summarize_gemini(session_id: str = Query(...), model: str | None = None, language: str = Query("Ø§Ù„Ø¹Ø±Ø¨ÙŠØ©")):
    model_key = model or DEFAULT_MODEL

    # Ù†Ø¨Ù†ÙŠ Ø§Ù„Ù€ prompt Ù„Ø§Ø­Ù‚Ø§Ù‹ Ø¨Ø¹Ø¯ Ø§Ù„ØªØ£ÙƒØ¯ Ù…Ù† ØªÙˆÙØ± Ø§Ù„Ù†Øµ
    def build_prompt(text: str) -> str:
        return (
        "Ø£Ù†Øª Ø®Ø¨ÙŠØ± ØªØ±Ø¨ÙˆÙŠ ÙˆÙ…ØµÙ…Ù… Ù…Ø­ØªÙˆÙ‰ Ø£ÙƒØ§Ø¯ÙŠÙ…ÙŠ Ù…ØªØ®ØµØµ ÙÙŠ ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ù†ØµÙˆØµ Ø§Ù„Ø¥Ø³Ù„Ø§Ù…ÙŠØ© ÙˆØ§Ù„Ø¹Ù„Ù…ÙŠØ© Ø¥Ù„Ù‰ Ø¯Ø±ÙˆØ³ ØªÙØ§Ø¹Ù„ÙŠØ© Ø¬Ø°Ø§Ø¨Ø©.\n\n"
        "### ğŸ¯ Ø§Ù„Ù‡Ø¯Ù:\n"
        "Ø£Ù†Ø´Ø¦ Ù…Ù„Ø®ØµÙ‹Ø§ Ø§Ø­ØªØ±Ø§ÙÙŠÙ‹Ø§ Ù„Ù„Ù†Øµ Ø§Ù„ØªØ§Ù„ÙŠ Ù„ÙŠØ¨Ø¯Ùˆ ÙƒØ¯Ø±Ø³ ØªØ¹Ù„ÙŠÙ…ÙŠ Ù…Ù†Ø¸Ù… Ù„Ù„Ø·Ù„Ø§Ø¨ ÙÙŠ Ù…ÙˆÙ‚Ø¹ Ø­Ø¯ÙŠØ«.\n\n"
        "### ğŸ§­ Ø§Ù„ØªØ¹Ù„ÙŠÙ…Ø§Øª:\n"
        "1. Ø§Ø³ØªØ®Ø¯Ù… ØªÙ†Ø³ÙŠÙ‚ **Markdown** Ø§Ù„ÙƒØ§Ù…Ù„:\n"
        "   - Ø§Ù„Ø¹Ù†ÙˆØ§Ù† Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… `#`.\n"
        "   - Ø§Ù„Ø¹Ù†Ø§ÙˆÙŠÙ† Ø§Ù„ÙØ±Ø¹ÙŠØ© Ù„Ù„Ø£ÙÙƒØ§Ø± Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… `##`.\n"
        "   - Ø§Ù„ÙÙ‚Ø±Ø§Øª Ø§Ù„Ù‚ØµÙŠØ±Ø© Ù„Ø§ ØªØªØ¬Ø§ÙˆØ² 4 Ø£Ø³Ø·Ø±.\n"
        "   - Ø§Ù„Ù‚ÙˆØ§Ø¦Ù… Ø§Ù„Ù…Ù†Ù‚Ø·Ø© ÙˆØ§Ù„Ù…Ø±Ù‚Ù…Ø© Ù„Ø¹Ø±Ø¶ Ø§Ù„Ù†Ù‚Ø§Ø· Ø¨ÙˆØ¶ÙˆØ­.\n"
        "   - Ø§Ø³ØªØ®Ø¯Ù… **bold** Ù„Ù„ÙƒÙ„Ù…Ø§Øª Ø§Ù„Ù…ÙØªØ§Ø­ÙŠØ© Ø§Ù„Ù…Ù‡Ù…Ø©.\n\n"
        "2. Ø§Ø¨Ø¯Ø£ Ø¨ÙÙ‚Ø±Ø© ØªÙ…Ù‡ÙŠØ¯ÙŠØ© Ø¬Ø°Ø§Ø¨Ø© Ø¨Ø¹Ù†ÙˆØ§Ù† **Ù…Ù„Ø®Øµ Ø³Ø±ÙŠØ¹** ØªØ´Ø±Ø­ Ø§Ù„ÙÙƒØ±Ø© Ø§Ù„Ø¹Ø§Ù…Ø© Ù„Ù„Ù†Øµ.\n"
        "3. Ø¨Ø¹Ø¯ Ø°Ù„ÙƒØŒ Ù†Ø¸Ù‘Ù… Ø§Ù„Ù…Ø­ØªÙˆÙ‰ Ø¥Ù„Ù‰ Ø£Ù‚Ø³Ø§Ù… ÙˆØ§Ø¶Ø­Ø© (Ù…Ø«Ù„Ø§Ù‹: **Ø£Ù‡Ù…ÙŠØ© Ø§Ù„Ù‚Ø±Ø¢Ù†**ØŒ **Ø§Ù„Ù†ØªØ§Ø¦Ø¬ Ø§Ù„Ù…ØªØ±ØªØ¨Ø© Ø¹Ù„Ù‰ Ø§Ù„Ø¥Ù‡Ù…Ø§Ù„**ØŒ **Ø¯Ø±ÙˆØ³ Ù…Ù† Ù‚ØµØ© Ø¢Ø¯Ù…** ... Ø¥Ù„Ø®).\n"
        "4. Ø£Ø¶Ù Ø±Ù…ÙˆØ²Ù‹Ø§ Ø£Ùˆ Ø¹Ù†Ø§ÙˆÙŠÙ† ÙØ±Ø¹ÙŠØ© Ù…Ø«Ù„: âœ¨ **Ø¯Ø±ÙˆØ³ Ù…Ø³ØªÙØ§Ø¯Ø©** Ø£Ùˆ ğŸ’¡ **ÙÙƒØ±Ø© Ø±Ø¦ÙŠØ³ÙŠØ©** Ù„Ø¥Ø¶ÙØ§Ø¡ Ø·Ø§Ø¨Ø¹ ØªÙØ§Ø¹Ù„ÙŠ.\n"
        "5. ÙÙŠ Ù†Ù‡Ø§ÙŠØ© Ø§Ù„Ù†ØµØŒ Ø£Ù†Ø´Ø¦ Ù‚Ø³Ù…Ù‹Ø§ Ø¨Ø¹Ù†ÙˆØ§Ù† `## Ø£Ø³Ø¦Ù„Ø© ÙˆØ£Ø¬ÙˆØ¨Ø©` ÙŠØ­ØªÙˆÙŠ Ù…Ù† 3 Ø¥Ù„Ù‰ 7 Ø£Ø³Ø¦Ù„Ø© ØªØ¯Ø±ÙŠØ¨ÙŠØ© ØªØ³Ø§Ø¹Ø¯ Ø§Ù„Ø·Ø§Ù„Ø¨ Ø¹Ù„Ù‰ Ø§Ù„ÙÙ‡Ù… ÙˆØ§Ù„Ù…Ø±Ø§Ø¬Ø¹Ø©.\n"
        "6. Ø§Ø¬Ø¹Ù„ Ø§Ù„Ø£Ø³Ù„ÙˆØ¨ **ØªØ¹Ù„ÙŠÙ…ÙŠÙ‹Ø§ Ù…Ø¨Ø³Ø·Ù‹Ø§** ÙˆÙ„ÙŠØ³ ÙˆØ¹Ø¸ÙŠÙ‹Ø§ØŒ Ù…Ø¹ Ù„Ù…Ø³Ø© Ø¥ÙŠÙ…Ø§Ù†ÙŠØ© Ù…Ù„Ù‡Ù…Ø©.\n"
        "7. Ù„Ø§ ØªÙƒØªØ¨ Ø£ÙŠ Ø´Ø±ÙˆØ­Ø§Øª Ø¥Ø¶Ø§ÙÙŠØ© Ø®Ø§Ø±Ø¬ Ø§Ù„Ù†Øµ.\n\n"
        f"### ğŸ§¾ Ø§Ù„Ù†Øµ Ø§Ù„Ù…Ø±Ø§Ø¯ ØªÙ„Ø®ÙŠØµÙ‡:\n{text}"
    )


    async def wait_for_text() -> str:
        text = text_storage.get(session_id)
        if text is not None:
            return text
        t = pending_extractions.get(session_id)
        if t is None:
            # Ù„Ø§ ÙŠÙˆØ¬Ø¯ Ù†Øµ ÙˆÙ„Ø§ Ù…Ù‡Ù…Ø© Ù…Ø¹Ù„Ù‘Ù‚Ø©
            raise HTTPException(status_code=404, detail="Ø§Ù„Ø¬Ù„Ø³Ø© ØºÙŠØ± Ù…ÙˆØ¬ÙˆØ¯Ø© Ø£Ùˆ Ø§Ù†ØªÙ‡Øª ØµÙ„Ø§Ø­ÙŠØªÙ‡Ø§")
        # Ø§Ù†ØªØ¸Ø± Ø§ÙƒØªÙ…Ø§Ù„ Ø§Ù„Ø§Ø³ØªØ®Ø±Ø§Ø¬ ÙˆÙ„ÙƒÙ† Ù„Ø§ Ù†Ø­Ø¬Ø¨ Ø§Ù„Ø­Ø¯Ø« Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠØ› Ø³ÙŠØªÙ… Ø§Ù„Ø§Ù†ØªØ¸Ø§Ø± Ø¯Ø§Ø®Ù„ Ø®ÙŠØ· Ø§Ù„Ø¨Ø«
        try:
            await t
        except Exception:
            pass
        return text_storage.get(session_id, "")

    async def sse_gen() -> AsyncGenerator[bytes, None]:
        # Ø¨Ø« ÙÙˆØ±ÙŠ Ù„Ø­Ø§Ù„Ø© Ø§Ù„Ø¨Ø¯Ø¡
        yield b"event: status\ndata: START\n\n"
        # Ø£Ø®Ø¨Ø± Ø§Ù„ÙˆØ§Ø¬Ù‡Ø© Ø£Ù† Ø§Ù„Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ù‚ÙŠØ¯ Ø§Ù„ØªØ­Ø¶ÙŠØ± Ø¥Ù† Ù„Ù… ÙŠÙƒÙ† Ø§Ù„Ù†Øµ Ø¬Ø§Ù‡Ø²Ø§Ù‹
        if session_id in pending_extractions and session_id not in text_storage:
            yield b"event: status\ndata: EXTRACTING\n\n"
        try:
            text_ready = await wait_for_text()

            if not (text_ready and text_ready.strip()):
                yield b"event: error\ndata: No extractable text found in PDF.\n\n"
                return

            # Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† Ø§Ù„ÙƒØ§Ø´ Ø­Ø³Ø¨ ØªØ¬Ø²Ø¦Ø© Ø§Ù„Ù†Øµ
            h = hashlib.sha256(text_ready.encode("utf-8")).hexdigest()
            now = time.time()
            cached = summary_cache.get(h)
            if cached and now - cached[0] < CACHE_TTL:
                log.info("cache hit for summary h=%s", h[:8])
                summary_text = cached[1]
                step = 800
                for i in range(0, len(summary_text), step):
                    chunk = summary_text[i:i+step]
                    yield _encode_sse_chunk(chunk)
                yield b"event: status\ndata: DONE\n\n"
                return

            # Ù„Ø§ ÙŠÙˆØ¬Ø¯ ÙƒØ§Ø´: Ø§Ø·Ù„Ø¨ Ù…Ù† Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ù…Ø¹ ØªØªØ¨Ø¹ Ø§Ù„Ø²Ù…Ù† ÙˆØ§Ø¬Ù…Ø¹ Ø§Ù„Ù†Ø§ØªØ¬ Ù„Ù„ØªØ®Ø²ÙŠÙ† Ù„Ø§Ø­Ù‚Ø§Ù‹
            prompt = build_prompt(text_ready)
            t0 = time.time()
            stream = client.models.generate_content_stream(
                model=model_key,
                contents=[prompt]
            )
            buf_parts: list[str] = []
            for chunk in stream:
                token = getattr(chunk, "text", None)
                if token:
                    buf_parts.append(token)
                    yield _encode_sse_chunk(token)
            full = "".join(buf_parts)
            summary_cache[h] = (time.time(), full)
            log.info("summary generated len=%d in %.2fs (cache key=%s)", len(full), time.time()-t0, h[:8])
            yield b"event: status\ndata: DONE\n\n"
        except HTTPException as he:
            yield f"event: error\ndata: {he.detail}\n\n".encode("utf-8")
        except Exception as e:
            log.exception("sse error: %s", e)
            yield f"event: error\ndata: {str(e)}\n\n".encode("utf-8")
    headers = {
        "Cache-Control": "no-cache",
        "X-Accel-Buffering": "no",
    }
    return StreamingResponse(sse_gen(), media_type="text/event-stream", headers=headers)
  